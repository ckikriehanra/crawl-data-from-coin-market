{
  "metadata": {
    "name": "test-quantity-data",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\npip install boto3"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nimport boto3\nimport pandas as pd\nimport pyarrow.parquet as pq\n\n# Set up MinIO client\ns3 \u003d boto3.client(\n    \"s3\",\n    endpoint_url\u003d\"http://minio1:9000\",  # Replace with your MinIO endpoint URL\n    aws_access_key_id\u003d\"kirihara\",       # Replace with your MinIO access key\n    aws_secret_access_key\u003d\"minioadmin\"  # Replace with your MinIO secret key\n)\n\n# List objects in a bucket\nbucket_name \u003d \"cken-coins-data\"  # Replace with your bucket name\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nsilver_folder_path \u003d \"silver/delta_table/price_coins_follow_minute/\"    # Replace with the path to your folder in the bucket\n\n# List all objects (files) in the folder\nresponse \u003d s3.list_objects_v2(Bucket\u003dbucket_name, Prefix\u003dsilver_folder_path)\n\n# Read Parquet files into a list of Pandas DataFrames\ndfs \u003d []\nfor obj in response[\u0027Contents\u0027]:\n    file_path \u003d obj[\u0027Key\u0027]\n    if file_path.endswith(\u0027.parquet\u0027):\n        parquet_obj \u003d s3.get_object(Bucket\u003dbucket_name, Key\u003dfile_path)\n        parquet_file \u003d pq.ParquetFile(BytesIO(parquet_obj[\u0027Body\u0027].read()))\n        df \u003d parquet_file.read().to_pandas()\n        dfs.append(df)\n\n# Concatenate all DataFrames into a single DataFrame\ncombined_df \u003d pd.concat(dfs, ignore_index\u003dTrue)\n\n# Print the combined DataFrame\nprint(combined_df.head())"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ngold_minute_folder_path \u003d \"gold/fact/fact_minute_price/\"\n\nresponse \u003d s3.list_objects_v2(Bucket\u003dbucket_name, Prefix\u003dgold_minute_folder_path)\n\ndfs_gold_minute \u003d []\nfor obj in response[\u0027Contents\u0027]:\n    file_path \u003d obj[\u0027Key\u0027]\n    if file_path.endswith(\u0027.parquet\u0027):\n        parquet_obj \u003d s3.get_object(Bucket\u003dbucket_name, Key\u003dfile_path)\n        parquet_file \u003d pq.ParquetFile(BytesIO(parquet_obj[\u0027Body\u0027].read()))\n        df \u003d parquet_file.read().to_pandas()\n        dfs_gold_minute.append(df)\n# Concatenate all DataFrames into a single DataFrame\ncombined_df_gold_minute \u003d pd.concat(dfs_gold_minute, ignore_index\u003dTrue)\n\n# Print the combined DataFrame\nprint(combined_df_gold_minute.head())"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ngold_hour_folder_path \u003d \"gold/fact/fact_hour_price/\"\n\nresponse \u003d s3.list_objects_v2(Bucket\u003dbucket_name, Prefix\u003dgold_hour_folder_path)\n\ndfs_gold_hour \u003d []\nfor obj in response[\u0027Contents\u0027]:\n    file_path \u003d obj[\u0027Key\u0027]\n    if file_path.endswith(\u0027.parquet\u0027):\n        parquet_obj \u003d s3.get_object(Bucket\u003dbucket_name, Key\u003dfile_path)\n        parquet_file \u003d pq.ParquetFile(BytesIO(parquet_obj[\u0027Body\u0027].read()))\n        df \u003d parquet_file.read().to_pandas()\n        dfs_gold_hour.append(df)\n        \n# Concatenate all DataFrames into a single DataFrame\ncombined_df_gold_hour \u003d pd.concat(dfs_gold_hour, ignore_index\u003dTrue)\n\n# Print the combined DataFrame\nprint(combined_df_gold_hour.head())"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\ngold_day_folder_path \u003d \"gold/fact/fact_day_price/\"\n\nresponse \u003d s3.list_objects_v2(Bucket\u003dbucket_name, Prefix\u003dgold_day_folder_path)\n\ndfs_gold_day \u003d []\nfor obj in response[\u0027Contents\u0027]:\n    file_path \u003d obj[\u0027Key\u0027]\n    if file_path.endswith(\u0027.parquet\u0027):\n        parquet_obj \u003d s3.get_object(Bucket\u003dbucket_name, Key\u003dfile_path)\n        parquet_file \u003d pq.ParquetFile(BytesIO(parquet_obj[\u0027Body\u0027].read()))\n        df \u003d parquet_file.read().to_pandas()\n        dfs_gold_day.append(df)\n        \n# Concatenate all DataFrames into a single DataFrame\ncombined_df_gold_day \u003d pd.concat(dfs_gold_day, ignore_index\u003dTrue)\n\n# Print the combined DataFrame\nprint(combined_df_gold_day.head())"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nprint(\"no_rows_in_silver)layer: {}\".format(len(combined_df)))\nprint(\"no_rows_in_gold_layer_minute: {}\".format(len(combined_df_gold_minute)))\nprint(\"no_rows_in_gold_layer_hour: {}\".format(len(combined_df_gold_hour))) # this is data of previous hour\nprint(\"no_rows_in_gold_layer_day: {}\".format(len(combined_df_gold_day))) # this is data of previous day"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%python\n"
    }
  ]
}